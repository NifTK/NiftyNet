stages:
  - pylint_test
  - dev_test
  - pip_test
  - pip_publish

pylintjob:
  stage: pylint_test
  except:
    - 139-correct-toynet-demo-instructions
    - 148-publish-niftynet-v0-2-0-on-python-package-index-pypi
    - 170-add-niftynet-paper-on-rtd-doc
    - 167-document-cli-option-for-path-to-new-networks
  script:
    - pylint --rcfile=tests/pylintrc niftynet/engine

testjob:
  stage: dev_test
  only:
    - master
    - dev
    - dev-staging
    - tags
  script:
    # !!kill coverage in case of hanging processes
    - if pgrep coverage; then pkill -f coverage; fi

    # print system info
    - which nvidia-smi
    - nvidia-smi
    - pwd
    - python -c "import tensorflow as tf; print tf.__version__"
    - python -c "import tensorflow as tf; from tensorflow.python.client import device_lib; print device_lib.list_local_devices()"
    - ls -la /dev | grep nvidia

    - echo $(python tests/get_gpu_index.py)
    - export CUDA_VISIBLE_DEVICES=$(python tests/get_gpu_index.py)

    # download data
    # - wget -q https://www.dropbox.com/s/y7mdh4m9ptkibax/example_volumes.tar.gz
    # - tar -xzvf example_volumes.tar.gz
    - wget -q https://www.dropbox.com/s/lioecnpv82r5n6e/example_volumes_v0_2.tar.gz
    - tar -xzvf example_volumes_v0_2.tar.gz
    # - wget -q https://www.dropbox.com/s/94wa4fl8f8k3aie/testing_data.tar.gz
    # - tar -xzvf testing_data.tar.gz
    - wget -q https://www.dropbox.com/s/p7b3t2c3mewtree/testing_data_v0_2.tar.gz
    - tar -xzvf testing_data_v0_2.tar.gz

    #### python 3 tests ###################################
    # save NiftyNet folder path just in case
    - export niftynet_dir=$(pwd)

    # create a virtual env to dev-test
    - venv="niftynet-dev-test-py3"
    - mypython=$(which python3)
    - virtualenv -p $mypython $venv
    - cd $venv
    - venv_dir=$(pwd)
    - source bin/activate
    # print Python version to CI output
    - which python
    - python --version
    - cd $niftynet_dir

    - pip install -r requirements-gpu.txt

    # tests
    - python net_segment.py train -c config/highres3dnet_config.ini --batch_size=1 --num_threads=2 --queue_length=40 --max_iter=10
    - python net_segment.py inference -c config/highres3dnet_config.ini --batch_size 8 --spatial_window_size 64,64,64 --queue_length 64

    - python net_segment.py train -c config/scalenet_config.ini --batch_size 1 --queue_length 5 --num_threads 2
    - python net_segment.py inference -c config/scalenet_config.ini --batch_size 16 --spatial_window_size 64,64,64 --queue_length 32

    - python net_segment.py train -c config/vnet_config.ini --batch_size 1 --queue_length 5 --num_threads 2 --activation_function relu
    - python net_segment.py inference -c config/vnet_config.ini --batch_size 16 --spatial_window_size 64,64,64 --queue_length 32 --activation_function relu

    # need a large GPU to run
    #- python net_segment.py train -c config/unet_config.ini --batch_size 1 --queue_length 5 --num_threads 2
    #- python net_segment.py inference -c config/unet_config.ini --batch_size 1 --spatial_window_size 96,96,96 --queue_length 5

    #- python net_segment.py train -c config/deepmedic_config.ini --batch_size 128 --queue_length 48 --num_threads 4
    #- python net_segment.py inference -c config/deepmedic_config.ini --batch_size 12 --spatial_window_size 135,135,135  --queue_length 128

    - python net_segment.py train -c config/default_segmentation.ini --batch_size 3 --queue_length 6
    - python net_segment.py train -c config/default_segmentation.ini --batch_size 3 --queue_length 6 --starting_iter 0 --max_iter 15
    - python net_segment.py inference -c config/default_segmentation.ini --spatial_window_size 84,84,84 --batch_size 7 --queue_length 14 --inference_iter 14

    - python net_segment.py train -c config/default_multimodal_segmentation.ini --batch_size 3
    - python net_segment.py inference -c config/default_multimodal_segmentation.ini --spatial_window_size 64,64 --batch_size 7

    - python net_regress.py train -c config/default_monomodal_regression.ini --batch_size=1 --name toynet --max_iter 10
    - python net_regress.py inference -c config/default_monomodal_regression.ini --batch_size=7 --name toynet --spatial_window_size 84,84,84

    - python net_gan.py train -c config/GAN_demo_train_config.ini --max_iter 5
    - python net_gan.py inference -c config/GAN_demo_train_config.ini

    - python net_autoencoder.py train -c config/vae_config.ini --max_iter 5
    - python net_autoencoder.py inference -c config/vae_config.ini --inference_type sample
    - python net_autoencoder.py inference -c config/vae_config.ini --inference_type encode --save_seg_dir output/vae_demo_features
    - python net_autoencoder.py inference -c config/vae_config.ini --inference_type encode-decode

    - python -m unittest discover -s "tests" -p "*_test.py"

    # deactivate virtual environment
    - deactivate
    - cd $niftynet_dir
    ###############end of python3

    ######### Python 2 ###################### run python2 code with coverage wrapper
    - coverage run -a --source . net_segment.py train -c config/highres3dnet_config.ini --batch_size=1 --num_threads=2 --queue_length=40 --max_iter=10
    - coverage run -a --source . net_segment.py inference -c config/highres3dnet_config.ini --batch_size 8 --spatial_window_size 64,64,64 --queue_length 64

    - coverage run -a --source . net_segment.py train -c config/scalenet_config.ini --batch_size 1 --queue_length 5 --num_threads 2
    - coverage run -a --source . net_segment.py inference -c config/scalenet_config.ini --batch_size 16 --spatial_window_size 64,64,64 --queue_length 32

    - coverage run -a --source . net_segment.py train -c config/vnet_config.ini --batch_size 1 --queue_length 5 --num_threads 2 --activation_function relu
    - coverage run -a --source . net_segment.py inference -c config/vnet_config.ini --batch_size 16 --spatial_window_size 64,64,64 --queue_length 32 --activation_function relu

    # need a large GPU to run
      #- coverage run -a --source . net_segment.py train -c config/unet_config.ini --batch_size 1 --queue_length 5 --num_threads 2
      #- coverage run -a --source . net_segment.py inference -c config/unet_config.ini --batch_size 1 --spatial_window_size 96,96,96 --queue_length 5

      #- coverage run -a --source . net_segment.py train -c config/deepmedic_config.ini --batch_size 128 --queue_length 48 --num_threads 4
      #- coverage run -a --source . net_segment.py inference -c config/deepmedic_config.ini --batch_size 12 --spatial_window_size 135,135,135  --queue_length 128

    - coverage run -a --source . net_segment.py train -c config/default_segmentation.ini --batch_size 3 --queue_length 6
    - coverage run -a --source . net_segment.py train -c config/default_segmentation.ini --batch_size 3 --queue_length 6 --starting_iter 0 --max_iter 15
    - coverage run -a --source . net_segment.py inference -c config/default_segmentation.ini --spatial_window_size 84,84,84 --batch_size 7 --queue_length 14 --inference_iter 14

    - coverage run -a --source . net_segment.py train -c config/default_multimodal_segmentation.ini --batch_size 3
    - coverage run -a --source . net_segment.py inference -c config/default_multimodal_segmentation.ini --spatial_window_size 64,64 --batch_size 7

    - coverage run -a --source . net_regress.py train -c config/default_monomodal_regression.ini --max_iter 10 --name toynet --batch_size=2
    - coverage run -a --source . net_run.py train -a net_regress -c config/default_monomodal_regression.ini --max_iter 10 --name toynet --batch_size=2
    - coverage run -a --source . net_regress.py inference -c config/default_monomodal_regression.ini --name toynet --spatial_window_size 84,84,84 --batch_size 7

    - coverage run -a --source . net_gan.py train -c config/GAN_demo_train_config.ini --max_iter 5
    - coverage run -a --source . net_run.py train -a net_gan -c config/GAN_demo_train_config.ini --max_iter 5
    - coverage run -a --source . net_gan.py inference -c config/GAN_demo_train_config.ini

    - coverage run -a --source . net_autoencoder.py train -c config/vae_config.ini --max_iter 5
    - coverage run -a --source . net_autoencoder.py inference -c config/vae_config.ini --inference_type sample
    - coverage run -a --source . net_autoencoder.py inference -c config/vae_config.ini --inference_type encode --save_seg_dir output/vae_demo_features
    - coverage run -a --source . net_autoencoder.py inference -c config/vae_config.ini --inference_type encode-decode

    - coverage run -a --source . -m unittest discover -s "tests" -p "*_test.py"
    - coverage report -m
    - echo 'finished test'
  tags:
    - gift-linux

quicktest:
  stage: dev_test
  except:
    - master
    - dev
    - dev-staging
    - tags
    - 177-merging-net_regress-to-dev
    - 112-publish-api-docs-online
    - 147-revise-contribution-guidelines-to-include-github
    - 150-properly-format-the-bibtex-entry-to-the-ipmi-2017-paper-on-the-main-readme
    - 139-correct-toynet-demo-instructions
    - 148-publish-niftynet-v0-2-0-on-python-package-index-pypi
    - 170-add-niftynet-paper-on-rtd-doc
    - 167-document-cli-option-for-path-to-new-networks
  script:
    # print system info
    - which nvidia-smi
    - nvidia-smi
    - pwd
    - python -c "import tensorflow as tf; print tf.__version__"
    - python -c "import tensorflow as tf; from tensorflow.python.client import device_lib; print device_lib.list_local_devices()"
    - ls -la /dev | grep nvidia

    - echo $(python tests/get_gpu_index.py)
    - export CUDA_VISIBLE_DEVICES=$(python tests/get_gpu_index.py)

    # download data
    # - wget -q https://www.dropbox.com/s/y7mdh4m9ptkibax/example_volumes.tar.gz
    # - tar -xzvf example_volumes.tar.gz
    - wget -q https://www.dropbox.com/s/lioecnpv82r5n6e/example_volumes_v0_2.tar.gz
    - tar -xzvf example_volumes_v0_2.tar.gz
      #- wget -q https://www.dropbox.com/s/94wa4fl8f8k3aie/testing_data.tar.gz
      #- tar -xzvf testing_data.tar.gz
    - wget -q https://www.dropbox.com/s/p7b3t2c3mewtree/testing_data_v0_2.tar.gz
    - tar -xzvf testing_data_v0_2.tar.gz

    - coverage erase
    # run only fast tests
    - QUICKTEST=True coverage run -a --source . -m unittest discover -s "tests" -p "*_test.py"
    - coverage report -m

    - echo 'finished quick tests'
  tags:
    - gift-linux


pip-installer:
  stage: pip_test
  only:
    - master
    - dev
    - dev-staging
    - tags
    - 177-merging-net_regress-to-dev
  script:
    # source utils
    - source ci/utils.sh
    # following three lines copied over from dev script:
    - ls -la /dev | grep nvidia
    - echo $(python tests/get_gpu_index.py)
    - export CUDA_VISIBLE_DEVICES=$(python tests/get_gpu_index.py)
    # create a Python file that will import all available packages from the pip installer
    - package_importer="$(pwd)/import_niftynet_packages.py"
    # traverse the file hierarchy recursively to discover all packages
    - find niftynet -type f \( ! -name . \) -print | grep '.py$' | grep -v __init__ | sed 's/\.\.\///g;s/\//\./g;s/\.py//g;s/^niftynet/import niftynet/g' > $package_importer
    # save NiftyNet folder path just in case
    - export niftynet_dir=$(pwd)
    # create the NiftyNet wheel
    - rm -rf dist  # remove dist directory, just in case
    - sh ci/bundlewheel.sh
    - source ci/findwheel.sh
    - echo $niftynet_wheel
    # ============= Python 2 ============================
    # create a virtual env to test pip installer
    - venv="niftynet-pip-installer-venv-py2"
    - mypython=$(which python2)
    - virtualenv -p $mypython $venv
    - cd $venv
    - venv_dir=$(pwd)
    - source bin/activate
    # print Python version to CI output
    - which python
    - python --version
    # NiftyNet console entries should fail gracefully if TF not installed
    # i.e. check that the warning displays the TF website
    - cd $niftynet_dir
    - set +e
    - python -c "import niftynet" 2>&1 | grep "https://www.tensorflow.org/"
    - set -e
    - cd $venv_dir
    # install TF
    - pip install tensorflow-gpu==1.2
    # install using built NiftyNet wheel
    - pip install $niftynet_wheel
    # install SimpleITK for package importer test to work properly
    - pip install simpleitk
    # check whether all packages are importable
    - cat $package_importer
    - python $package_importer
    # test niftynet command
    - ln -s /home/gitlab-runner/environments/niftynet/data/example_volumes ./example_volumes
    - net_segment train -c $niftynet_dir/config/default_segmentation.ini --name toynet --batch_size 3 --max_iter 5
    - net_segment inference -c $niftynet_dir/config/default_segmentation.ini --name toynet --spatial_window_size 80,80,80 --batch_size 8
    - net_run train --app net_segment -c $niftynet_dir/config/default_segmentation.ini --name toynet --batch_size 3 --max_iter 5
    - net_run inference --app net_segment -c $niftynet_dir/config/default_segmentation.ini --name toynet --spatial_window_size 80,80,80 --batch_size 8

    - net_regress train -c $niftynet_dir/config/default_monomodal_regression.ini --max_iter 10 --name toynet --batch_size=2
    - net_regress inference -c $niftynet_dir/config/default_monomodal_regression.ini --name toynet --spatial_window_size 84,84,84 --batch_size 7
    - net_run train -a net_regress -c $niftynet_dir/config/default_monomodal_regression.ini --max_iter 10 --name toynet --batch_size=2
    - net_run inference -a net_regress -c $niftynet_dir/config/default_monomodal_regression.ini --name toynet --spatial_window_size 84,84,84 --batch_size 7

    - net_gan train -c $niftynet_dir/config/GAN_demo_train_config.ini --max_iter 5
    - net_gan inference -c $niftynet_dir/config/GAN_demo_train_config.ini
    - net_run train --app net_gan -c $niftynet_dir/config/GAN_demo_train_config.ini --max_iter 5
    - net_run inference --app net_gan -c $niftynet_dir/config/GAN_demo_train_config.ini

    - net_autoencoder train -c $niftynet_dir/config/vae_config.ini --max_iter 5
    - net_autoencoder inference -c $niftynet_dir/config/vae_config.ini --inference_type sample
    - net_autoencoder inference -c $niftynet_dir/config/vae_config.ini --inference_type encode --save_seg_dir output/vae_demo_features
    - net_autoencoder inference -c $niftynet_dir/config/vae_config.ini --inference_type encode-decode
    - net_run train --app net_autoencoder -c $niftynet_dir/config/vae_config.ini --max_iter 5
    - net_run inference --app net_autoencoder -c $niftynet_dir/config/vae_config.ini --inference_type sample
    - net_run inference --app net_autoencoder -c $niftynet_dir/config/vae_config.ini --inference_type encode --save_seg_dir output/vae_demo_features
    - net_run inference --app net_autoencoder -c $niftynet_dir/config/vae_config.ini --inference_type encode-decode
    # deactivate virtual environment
    - deactivate
    - cd $niftynet_dir
    # ============= Python 3 ============================
    # create a virtual env to test pip installer
    - venv="niftynet-pip-installer-venv-py3"
    - mypython=$(which python3)
    - virtualenv -p $mypython $venv
    - cd $venv
    - venv_dir=$(pwd)
    - source bin/activate
    # print Python version to CI output
    - which python
    - python --version
    # NiftyNet console entries should fail gracefully if TF not installed
    # i.e. check that the warning displays the TF website
    - cd $niftynet_dir
    - set +e
    - python -c "import niftynet" 2>&1 | grep "https://www.tensorflow.org/"
    - set -e
    - cd $venv_dir
    # install TF
    - pip install tensorflow-gpu==1.2
    # install using built NiftyNet wheel
    - pip install $niftynet_wheel
    # install SimpleITK for package importer test to work properly
    - pip install simpleitk
    # check whether all packages are importable
    - cat $package_importer
    - python $package_importer
    # test niftynet command
    - ln -s /home/gitlab-runner/environments/niftynet/data/example_volumes ./example_volumes

    - net_segment train -c $niftynet_dir/config/default_segmentation.ini --name toynet --batch_size 3 --max_iter 5
    - net_segment inference -c $niftynet_dir/config/default_segmentation.ini --name toynet --spatial_window_size 80,80,80 --batch_size 8
    - net_run train --app net_segment -c $niftynet_dir/config/default_segmentation.ini --name toynet --batch_size 3 --max_iter 5
    - net_run inference --app net_segment -c $niftynet_dir/config/default_segmentation.ini --name toynet --spatial_window_size 80,80,80 --batch_size 8

    - net_regress train -c $niftynet_dir/config/default_monomodal_regression.ini --max_iter 10 --name toynet --batch_size=2
    - net_regress inference -c $niftynet_dir/config/default_monomodal_regression.ini --name toynet --spatial_window_size 84,84,84 --batch_size 7
    - net_run train -a net_regress -c $niftynet_dir/config/default_monomodal_regression.ini --max_iter 10 --name toynet --batch_size=2
    - net_run inference -a net_regress -c $niftynet_dir/config/default_monomodal_regression.ini --name toynet --spatial_window_size 84,84,84 --batch_size 7

    - net_gan train -c $niftynet_dir/config/GAN_demo_train_config.ini --max_iter 5
    - net_gan inference -c $niftynet_dir/config/GAN_demo_train_config.ini
    - net_run train --app net_gan -c $niftynet_dir/config/GAN_demo_train_config.ini --max_iter 5
    - net_run inference --app net_gan -c $niftynet_dir/config/GAN_demo_train_config.ini

    - net_autoencoder train -c $niftynet_dir/config/vae_config.ini --max_iter 5
    - net_autoencoder inference -c $niftynet_dir/config/vae_config.ini --inference_type sample
    - net_autoencoder inference -c $niftynet_dir/config/vae_config.ini --inference_type encode --save_seg_dir output/vae_demo_features
    - net_autoencoder inference -c $niftynet_dir/config/vae_config.ini --inference_type encode-decode
    - net_run train --app net_autoencoder -c $niftynet_dir/config/vae_config.ini --max_iter 5
    - net_run inference --app net_autoencoder -c $niftynet_dir/config/vae_config.ini --inference_type sample
    - net_run inference --app net_autoencoder -c $niftynet_dir/config/vae_config.ini --inference_type encode --save_seg_dir output/vae_demo_features
    - net_run inference --app net_autoencoder -c $niftynet_dir/config/vae_config.ini --inference_type encode-decode

    # deactivate virtual environment
    - deactivate
    - cd $niftynet_dir
  tags:
    - gift-adelie

pip-camera-ready:
  stage: pip_publish
  only:
    - tags
  script:
    # Copy wheel created in previous stage to a specific location on GIFT-Adelie
    - export niftynet_dir=$(pwd)
    # create the NiftyNet wheel
    - rm -rf dist  # remove dist directory, just in case
    - sh ci/bundlewheel.sh
    - source ci/findwheel.sh
    - echo $niftynet_wheel
    # Creat camera-ready folder if doesn't exist
    - camera_ready_dir=/home/gitlab-runner/environments/niftynet/pip/camera-ready
    - mkdir -p $camera_ready_dir
    - ls -lrtha $camera_ready_dir
    # Clean up the camera-ready folder if already there
    - rm -rf $camera_ready_dir/*.whl
    # Finally do copy
    - cp $niftynet_wheel $camera_ready_dir
    - ls -lrtha $camera_ready_dir
    # Instruct developer which file to publish
    - echo "Camera-ready pip installer bundle (wheel) created:"
    - echo "$(ls $camera_ready_dir/*.whl)"
  tags:
    - gift-adelie
